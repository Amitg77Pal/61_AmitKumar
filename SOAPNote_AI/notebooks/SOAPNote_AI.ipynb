{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SOAPNote AI\n",
    "## Doctor Dictation to Structured Clinical Notes\n",
    "Convert audio dictations into structured SOAP notes using Whisper and LangChain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "from typing import Optional\n",
    "from faster_whisper import WhisperModel\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "# Ensure you have your OPENAI_API_KEY set in environment or uncomment below\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"sk-...\"\n",
    "\n",
    "MODEL_SIZE = \"tiny\" # linear or base for speed on CPU/Mac\n",
    "AUDIO_PATH = \"../data/sample_dictation.mp3\"\n",
    "DEVICE = \"cpu\" # or \"cuda\" if available\n",
    "COMPUTE_TYPE = \"int8\" # or \"float16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Transcription Logic\n",
    "def transcribe_audio(audio_path, model_size=\"base\", device=\"cpu\", compute_type=\"int8\"):\n",
    "    print(f\"Loading Whisper model: {model_size}...\")\n",
    "    model = WhisperModel(model_size, device=device, compute_type=compute_type)\n",
    "    \n",
    "    print(f\"Transcribing {audio_path}...\")\n",
    "    segments, info = model.transcribe(audio_path, beam_size=5)\n",
    "    \n",
    "    transcript = \"\"\n",
    "    for segment in segments:\n",
    "        transcript += segment.text + \" \"\n",
    "    \n",
    "    return transcript.strip()\n",
    "\n",
    "# Test Transcription (Uncomment to run independently)\n",
    "# text = transcribe_audio(AUDIO_PATH, MODEL_SIZE, DEVICE, COMPUTE_TYPE)\n",
    "# print(\"Transcript:\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Define SOAP Schema\n",
    "class SOAPNote(BaseModel):\n",
    "    Subjective: str = Field(description=\"Patient's subjective report of symptoms, history, and complaints.\")\n",
    "    Objective: str = Field(description=\"Objective findings, vital signs, physical exam results, labs.\")\n",
    "    Assessment: str = Field(description=\"Diagnosis or differential diagnosis based on findings.\")\n",
    "    Plan: str = Field(description=\"Treatment plan, medications, follow-up, and further testing.\")\n",
    "\n",
    "# 3. Structuring Logic\n",
    "def structure_soap_note(transcript, model_name=\"gpt-3.5-turbo\"):\n",
    "    # Initialize LLM\n",
    "    llm = ChatOpenAI(model=model_name, temperature=0)\n",
    "    \n",
    "    # Define Parser\n",
    "    parser = JsonOutputParser(pydantic_object=SOAPNote)\n",
    "    \n",
    "    # Prompt\n",
    "    prompt = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", \"You are a helpful medical assistant. Your task is to extract a structured SOAP note from a doctor's dictation. \\nEnsure the output matches the following JSON schema: {format_instructions}\\nDisclaimer: For clinical documentation assistance only. Not a medical decision system.\"),\n",
    "        (\"user\", \"Dictation: {transcript}\")\n",
    "    ])\n",
    "    \n",
    "    chain = prompt | llm | parser\n",
    "    \n",
    "    result = chain.invoke({\"transcript\": transcript, \"format_instructions\": parser.get_format_instructions()})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Full Pipeline Execution\n",
    "def run_soap_pipeline(audio_path):\n",
    "    # Step 1: Transcribe\n",
    "    print(\"--- Starting Transcription ---\")\n",
    "    transcript = transcribe_audio(audio_path, MODEL_SIZE, DEVICE, COMPUTE_TYPE)\n",
    "    print(\"\\n[Transcript]:\")\n",
    "    print(transcript)\n",
    "    \n",
    "    # Step 2: Structure\n",
    "    print(\"\\n--- Structuring SOAP Note ---\")\n",
    "    # Note: Requires OPENAI_API_KEY. If not present, this will fail.\n",
    "    try:\n",
    "        soap_note = structure_soap_note(transcript)\n",
    "        print(\"\\n[SOAP Note JSON]:\")\n",
    "        print(soap_note)\n",
    "        \n",
    "        print(\"\\n--- Formatted Output ---\")\n",
    "        for key, value in soap_note.items():\n",
    "            print(f\"{key}:\\n{value}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"Structuring failed (likely missing API Key): {e}\")\n",
    "        print(\"Using mock output for demo if API fails:\")\n",
    "        print(f\"Subjective: {transcript}\")\n",
    "        \n",
    "run_soap_pipeline(AUDIO_PATH)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
